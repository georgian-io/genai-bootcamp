{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashsaravanan/Library/Caches/pypoetry/virtualenvs/genai-bootcamp-4wh1UwyX-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", embed_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the index from disk\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_store = LanceDBVectorStore(\n",
    "    uri=\"./lancedb\", table_name=\"pipeline_test\"\n",
    ")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(index, user_input):\n",
    "    results = index.as_retriever(similarity_top_k=3).retrieve(user_input)\n",
    "    return \"\\n----------------\\n\".join([f\"Title: {result.metadata['title']}\\n{result.text}\" for result in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Set up OpenAI client - API key is handled in your .env file\n",
    "openai_llm = OpenAI(model=\"gpt-4o\", temperature=0.1, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnyScale\n",
    "from llama_index.llms.anyscale import Anyscale\n",
    "\n",
    "# Set up AnyScale client - API key is handled in your .env file\n",
    "anyscale_llm = Anyscale(model=\"meta-llama/Meta-Llama-3-70B-Instruct\", temperature=0.1, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyDE\n",
    "\n",
    "We examine two scenarios - one where we don't use HyDE and one where we do. \n",
    "\n",
    "Note: To focus on HyDE, we've simplified and abstracted away most of the helper code we used previously.\n",
    "\n",
    "## Without Hyde\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"How many points did Michael Jordan actually score in his final NBA game?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "prompt = \"\"\"You are a helpful AI assistant that answers questions after carefully reading all the provided context. You always cite sources (document titles) and also quote the relevant snippets. Information may be spread across multiple documents. If the information is not present in any of the contexts, you will say 'I don't know'.\n",
    "-----\n",
    "Context: \n",
    "{context_str} \n",
    "-----\n",
    "Question: {query_str} \n",
    "Answer: `answer`\n",
    "Source: `source document title`\n",
    "Relevant Snippet: `snippet`\n",
    "\"\"\"\n",
    "\n",
    "# Convert prompt into a prompt template that llamaindex can use\n",
    "prompt = PromptTemplate(template=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "openai_query_engine = index.as_query_engine(llm=openai_llm, text_qa_template=prompt, similarity_top_k=3)\n",
    "\n",
    "# Anyscale\n",
    "anyscale_query_engine = index.as_query_engine(llm=anyscale_llm, text_qa_template=prompt, similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Jordan scored 13 points in his final NBA game.\n",
      "\n",
      "Source: Michael Jordan\n",
      "Relevant Snippet: \"Jordan's final NBA game was on April 16, 2003 in Philadelphia. After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and with his team trailing the Philadelphia 76ers, 75–56.\"\n"
     ]
    }
   ],
   "source": [
    "print(openai_query_engine.query(user_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 13\n",
      "Source: Michael Jordan\n",
      "Relevant Snippet: \"After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and with his team trailing the Philadelphia 76ers, 75–56.\"\n"
     ]
    }
   ],
   "source": [
    "print(anyscale_query_engine.query(user_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query transforms perform query transformations such as expansion, HyDE, etc.\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "openai_hyde = HyDEQueryTransform(llm=openai_llm, include_original=True)\n",
    "anyscale_hyde = HyDEQueryTransform(llm=anyscale_llm, include_original=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query engine applies a query transformation before retrieval/answer generation\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "anyscale_hyde_query_engine = TransformQueryEngine(anyscale_query_engine, anyscale_hyde)\n",
    "openai_hyde_query_engine = TransformQueryEngine(openai_query_engine, anyscale_hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write a passage to answer the question\n",
      "Try to include as many key details as possible.\n",
      "\n",
      "\n",
      "{context_str}\n",
      "\n",
      "\n",
      "Passage:\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the HyDE prompt\n",
    "print(anyscale_hyde.get_prompts()[\"hyde_prompt\"].template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothetical document:\n",
      "Here is a passage answering the question:\n",
      "\n",
      "Michael Jordan's final NBA game took place on April 18, 2003, when his Washington Wizards faced off against the Philadelphia 76ers in the first round of the playoffs. In a game that would ultimately be a 107-87 loss for the Wizards, Jordan, then 40 years old, played 37 minutes and scored 15 points. This marked the last time Jordan would step onto an NBA court as a player, bringing an end to an illustrious career that spanned nearly two decades and included six NBA championships, five MVP awards, and countless other accolades. Despite the disappointing outcome of the game, Jordan's 15-point performance was a testament to his enduring talent and competitive spirit, even in the twilight of his playing days.\n",
      "------------------------------------------------------------------------------------------\n",
      "Answer: 13 points\n",
      "Source: Michael Jordan\n",
      "Relevant Snippet: \"After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and with his team trailing the Philadelphia 76ers, 75–56.\"\n"
     ]
    }
   ],
   "source": [
    "# Anyscale\n",
    "print(\"Hypothetical document:\")\n",
    "print(anyscale_hyde.run(user_input).custom_embedding_strs[0])\n",
    "print(\"---\" * 30)\n",
    "print(anyscale_hyde_query_engine.query(user_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothetical document:\n",
      "OpenAI: Michael Jordan, widely regarded as one of the greatest basketball players of all time, played his final NBA game on April 16, 2003. At the age of 40, Jordan was playing for the Washington Wizards, having come out of retirement for a second time to join the team in 2001. His last game took place against the Philadelphia 76ers at the First Union Center in Philadelphia. Despite the Wizards losing the game 107-87, Jordan's performance was a memorable moment in sports history. In his final NBA appearance, Michael Jordan scored 15 points. He played 28 minutes, shooting 6-of-15 from the field and 3-of-4 from the free-throw line. The game was marked by a standing ovation from the crowd and heartfelt tributes from both teammates and opponents, celebrating the end of an illustrious career that included six NBA championships and five MVP awards.\n",
      "------------------------------------------------------------------------------------------\n",
      "Michael Jordan scored 13 points in his final NBA game.\n",
      "\n",
      "Source: Michael Jordan\n",
      "Relevant Snippet: \"Jordan's final NBA game was on April 16, 2003 in Philadelphia. After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and with his team trailing the Philadelphia 76ers, 75–56.\"\n"
     ]
    }
   ],
   "source": [
    "# OpenAI\n",
    "print(\"Hypothetical document:\")\n",
    "print(\"OpenAI:\", openai_hyde.run(user_input).custom_embedding_strs[0])\n",
    "print(\"---\" * 30)\n",
    "print(openai_hyde_query_engine.query(user_input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-bootcamp-4wh1UwyX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
