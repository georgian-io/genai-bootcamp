{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define metrics to use\n",
    "In general, we want to evaluate 2 things: whether the context retrieved is good, and whether the overall answer is good.\n",
    "\n",
    "We'll be tracking the following metrics, and for some we'll use the `ragas` library to evaluate. \n",
    "\n",
    "## Assessing the context\n",
    "\n",
    "We need to remember that our RAG system will retrieve multiple contexts. We use multiple metrics here to measure this retrieved context. This can be thought of as an evaluation of the Retrieval part of the system.\n",
    "\n",
    "- `context_correctness`: Whether the `ground_truth_context` is included in the list of retrieved contexts.\n",
    "- `ground_truth_context_rank`: The position or rank of the ground_truth_context in the retrieved context. \n",
    "- `context_rougel_score`: ROUGE-L score between the ground_truth_context and the top retrieved context.\n",
    "- `context_precision` (with `ragas`): How relevant the retrieved contexts are to the question (assessed with an LLM).\n",
    "\n",
    "## Assessing the answer\n",
    "\n",
    "In contrast to the above, here we evaluate the generated answers themselves. We use two metrics here:\n",
    "\n",
    "- `faithfulness` (with `ragas`): does the answer use information from the context? (assessed with an LLM)\n",
    "- `answer_correctness` (with `ragas`): a combination of the following\n",
    "    - how relevant is the answer to the question? based on cosine similarity of embeddings\n",
    "    - whether the answer matches with the ground truth, assessed with LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashsaravanan/Library/Caches/pypoetry/virtualenvs/genai-bootcamp-4wh1UwyX-py3.12/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "def context_correctness(ground_truth_context, contexts):\n",
    "    \"\"\"whether the `ground_truth_context` is included in the list of retrieved contexts\"\"\"\n",
    "    return ground_truth_context in contexts\n",
    "\n",
    "\n",
    "def ground_truth_context_rank(ground_truth_context, contexts):\n",
    "    \"\"\"rank of the ground_truth_context in the retrieved contexts, -1 if not found\"\"\"\n",
    "    if ground_truth_context is not None:\n",
    "        try:\n",
    "            return contexts.index(ground_truth_context)\n",
    "        except:\n",
    "            return -1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def context_rougel_score(ground_truth_context, contexts):\n",
    "    \"\"\"ROUGE-L score between the ground_truth_context and the top retrieved context\"\"\"\n",
    "    rouge = evaluate.load('rouge')\n",
    "    return rouge.compute(predictions=[contexts[0]], references=[ground_truth_context])[\"rougeL\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `ragas`, we can simply call them together as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate as ragas_evaluate\n",
    "from ragas.metrics import context_precision, faithfulness, answer_correctness\n",
    "from ragas.llms import LlamaIndexLLMWrapper\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "def evaluate_w_ragas(item, metrics=[context_precision, faithfulness, answer_correctness]):\n",
    "    evaluator_llm = LlamaIndexLLMWrapper(OpenAI(model=\"gpt-4o\", temperature=0.0, max_tokens=2048))\n",
    "\n",
    "    # Format the example into datasets, which ragas takes as inputs\n",
    "    row_dataset = Dataset.from_pandas(item.to_frame().T)\n",
    "\n",
    "    # Ragas by default takes in a batch of items and aggregates metrics together\n",
    "    # So in this example, we need to pass one by one to get individual results.\n",
    "    # However, it does run faster when you pass all metrics at once.\n",
    "    ragas_eval_results = ragas_evaluate(row_dataset, metrics=metrics, llm=evaluator_llm, embeddings=None)\n",
    "    return ragas_eval_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
