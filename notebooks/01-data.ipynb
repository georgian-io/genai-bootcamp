{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "For this example we use the [Wikipedia](https://huggingface.co/datasets/wikipedia) dataset. Specifically, we use \"Simple Wikipedia\", due to its smaller size. The link also describes other variants of Wikipedia the dataset contains (including alternate languages and the full English Wikipedia).\n",
    "\n",
    "Our dataset contains 205,328 documents but we sample 1000 for the purposes of this demo. Each datapoint contains the url of the page, the title of the page, and the text available on that page. For our example, we store the url and title as metadata and explicitly do not send the url to either the embedding model or the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"wikipedia\", \"20220301.simple\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "np.random.seed(42)\n",
    "data = data[\"train\"][np.random.choice(data[\"train\"].shape[0], size=sample_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://simple.wikipedia.org/wiki/Robert%20Wagner\n",
      "Title: Robert Wagner\n",
      "Text: Robert John Wagner, Jr. (born February 10, 1930 in Detroit, Michigan, U.S.), is an American actor. His paternal grandparents were from Germany.\n",
      "\n",
      "Early career \n",
      "His career began in 1950 as an extra in the movie The Happy Years. Then he got a role in the war films Halls of Montezuma (1951), and The Frogmen (1951), with Richard Widmark, What Price Glory? (1952) with James Cagney and directed by John Ford.\n",
      "\n",
      "In 1953 he was nominated for a Golden Globe for Stars and Stripes Forever (1952).\n",
      "\n",
      "Then would ...\n"
     ]
    }
   ],
   "source": [
    "data_index = 58\n",
    "print(data[\"url\"][data_index] + \"\\nTitle: \" + data[\"title\"][data_index] + \"\\nText: \" + data[\"text\"][data_index][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 24050.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# We load our dataset into a list of llamaindex documents\n",
    "from llama_index.core import Document\n",
    "\n",
    "documents = []\n",
    "for i in tqdm.tqdm(range(len(data[\"text\"]))):\n",
    "    documents.append(\n",
    "        Document(\n",
    "            text=data[\"text\"][i],\n",
    "            metadata={\"title\": data[\"title\"][i], \"url\": data[\"url\"][i]},\n",
    "            excluded_embed_metadata_keys=[\"url\"], # We don't embed the url\n",
    "            excluded_llm_metadata_keys=[\"url\"], # We don't send the url to LLM\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='2d1934ad-ff3b-477e-bf75-ae53ffbc8609', embedding=None, metadata={'title': 'Halal snack pack', 'url': 'https://simple.wikipedia.org/wiki/Halal%20snack%20pack'}, excluded_embed_metadata_keys=['url'], excluded_llm_metadata_keys=['url'], relationships={}, text='A halal snack pack, or HSP, is a dish that comes from Australia. It is made up of halal-certified doner kebab meat (mainly lamb, chicken or beef), chips, sauces (mainly chili, garlic and barbecue sauces) and often cheese. The exact origin of the halal snack pack is unknown. Halal snack packs have become more popular since 2015, when the Facebook group \"Halal Snack Pack Appreciation Society\" was created.\\n\\nReferences\\n\\nAustralia\\nFast food', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "**Data Cleaning:** In the case of this dataset, data is extracted at a particular point in time, so there's no chances of old or conflicting information. \n",
    "\n",
    "**Data Extraction:** In addition, as this is a preprocessed dataset used by the general community, it is also clean with no real need to optimize the extraction.\n",
    "\n",
    "**PII:** Wikipedia does not contain PII.\n",
    "\n",
    "**Data Enrichment:** While we don't use this information in this demo, we can store the url and page titles as part of the metadata. Given this information already exists, we just store it alongside our embeddings further on in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "We first demonstrate basic fixed size chunking with overlaps via llamaindex's [TokenTextSplitter](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/token_text_splitter/).\n",
    "\n",
    "However, for the rest of the demo, we use a mix of two chunking methods - fixed-size chunking and content-aware chunking. Specifically, we use llamaindex's [SentenceSplitter](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_splitter/) which attempts to keep sentences and paragraphs together while maintaining chunks of roughly equal size.\n",
    "\n",
    "We use a chunk size of 512 which is the maximum size that our embedding model accepts. Within these chunks, we set an overlap of 20 tokens which corresponds to roughly 1-2 sentences.\n",
    "\n",
    "You can examine other chunking methods implemented in llamaindex [here](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Size Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "chunker = TokenTextSplitter(chunk_size=512, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document has ~581 words and was split into 2 chunks of maximum 512 tokens.\n"
     ]
    }
   ],
   "source": [
    "sample = data[\"text\"][data_index]\n",
    "chunked = chunker.split_text(sample)\n",
    "print(f\"Document has ~{len(sample.split(' '))} words and was split into {len(chunked)} chunks of maximum 512 tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Robert John Wagner, Jr. (born February 10, 1930 in Detroit, Michigan, U.S.), is an American actor. His paternal grandparents were from Germany.\\n\\nEarly career \\nHis career began in 1950 as an extra in the movie The Happy Years. Then he got a role in the war films Halls of Montezuma (1951), and The Frogmen (1951), with Richard Widmark, What Price Glory? (1952) with James Cagney and directed by John Ford.\\n\\nIn 1953 he was nominated for a Golden Globe for Stars and Stripes Forever (1952).\\n\\nThen would have outstanding performances in movies as Prince Valiant (1954) directed by Henry Hathaway and the Western, Broken Lance (1954) with Spencer Tracy and directed by Edward Dmytryk. \\n\\nIn 1955 he obtained his first starred in the Western White Feather (1955), followed by Film-Noir A Kiss Before Dying (1956) and war film Between Heaven and Hell (1956) directed by Richard Fleischer.\\n\\nWagner played the legendary gunslinger Jesse James in the movie The True Story of Jesse James (1957) directed by Nicholas Ray\\n\\nHis works in the 1960's include The Longest Day (1962), The War Lover (1962) with Steve McQueen, The Pink Panther (1963) with Peter Sellers, and Banning (1967)\\n\\n1970s - 1980s\\nIn 1974 Wagner, plays of role of Bigelow, one of the people trapped in burning building in The Towering Inferno, with Steve McQueen and Paul Newman and directed by John Guillermin.\\n\\nAnother of his works in the 70s was in the war movie directed by Jack Smight, Midway (1976) as Lieutenant Commander Ernest L. Blake \\n\\nHe also co-starred in The Concorde... Airport '79 (1979) with Alain Delon.\\n\\nIn the 1980 he gets a few roles in movies like Curse of the Pink Panther (1983) directed by Blake Edwards, and I Am the Cheese (1983).\\n\\n1990s \\nThe 1990s were very good for him getting the role of Number Two in Austin Powers (1997) with Mike Myers.\\nWagner returned to play the characters in Austin Powers: The Spy Who Shagged Me (1999), and Austin Powers in Goldmember (2002).\\n\\nIn the last decade, Wagner has worked in movies El\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can result in sentences being cut off in the middle\n",
    "chunked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Austin Powers in Goldmember (2002).\\n\\nIn the last decade, Wagner has worked in movies El padrino (2004), Hoot (2006), Man in the Chair (2007) with Christopher Plummer, The Wild Stallion (2009).\\n\\nTelevision \\nRobert Wagner starred in three successful series for American television. One of them was It Takes a Thief, Alexander Mundy a spy performing dangerous missions for the government of the United States. I also work in the series Fred Astaire as Alistair Mundy.\\n\\nWagner starred in the series for 66 episodes between 1968 and 1970. and was nominated for an Emmy and Golden Globe Awards in 1970\\n\\nIn 1975 Wagner stars with Eddie Albert, the series of detectives created by Glen A. Larson, Switch (1975 - 1978). he plays detective Pete T. Ryan.\\n\\nHis television series Hart to Hart (1979-1984), was most successful of all, Wagner is Jonathan Hart who with his wife Jennifer Hart, Stefanie Powers, two detectives who solved the most difficult criminal cases in high society.  \\n\\nThe series created by Sidney Sheldon, was an immediate success.\\n\\nMade several television movies including Hart to Hart, Hart to Hart Returns (1993), Hart to Hart: Home Is Where the Hart Is (1994), Hart to Hart: Crimes of the Hart (1994), Hart to Hart: Old Friends Never Die (1994), Hart to Hart: Two Harts in 3/4 Time (1995), Hart to Hart: Harts in High Season (1996) and Hart to Hart: Till Death Do Us Hart (1996).\\n\\nReferences\\n \\n \\n Robert Wagner on Yahoo! Movies\\n Articles about Robert Wagner, a Malibu resident, can be found at The Malibu Times \\n\\n1930 births\\nLiving people\\nActors from Detroit, Michigan\\nAmerican movie actors\\nAmerican television actors'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Aware Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "chunker = SentenceSplitter(chunk_size=512, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document has 581 words and was split into 2 chunks of maximum 512 tokens.\n"
     ]
    }
   ],
   "source": [
    "sample = data[\"text\"][data_index]\n",
    "chunked = chunker.split_text(sample)\n",
    "print(f\"Document has {len(sample.split(' '))} words and was split into {len(chunked)} chunks of maximum 512 tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Robert John Wagner, Jr. (born February 10, 1930 in Detroit, Michigan, U.S.), is an American actor. His paternal grandparents were from Germany.\\n\\nEarly career \\nHis career began in 1950 as an extra in the movie The Happy Years. Then he got a role in the war films Halls of Montezuma (1951), and The Frogmen (1951), with Richard Widmark, What Price Glory? (1952) with James Cagney and directed by John Ford.\\n\\nIn 1953 he was nominated for a Golden Globe for Stars and Stripes Forever (1952).\\n\\nThen would have outstanding performances in movies as Prince Valiant (1954) directed by Henry Hathaway and the Western, Broken Lance (1954) with Spencer Tracy and directed by Edward Dmytryk. \\n\\nIn 1955 he obtained his first starred in the Western White Feather (1955), followed by Film-Noir A Kiss Before Dying (1956) and war film Between Heaven and Hell (1956) directed by Richard Fleischer.\\n\\nWagner played the legendary gunslinger Jesse James in the movie The True Story of Jesse James (1957) directed by Nicholas Ray\\n\\nHis works in the 1960's include The Longest Day (1962), The War Lover (1962) with Steve McQueen, The Pink Panther (1963) with Peter Sellers, and Banning (1967)\\n\\n1970s - 1980s\\nIn 1974 Wagner, plays of role of Bigelow, one of the people trapped in burning building in The Towering Inferno, with Steve McQueen and Paul Newman and directed by John Guillermin.\\n\\nAnother of his works in the 70s was in the war movie directed by Jack Smight, Midway (1976) as Lieutenant Commander Ernest L. Blake \\n\\nHe also co-starred in The Concorde... Airport '79 (1979) with Alain Delon.\\n\\nIn the 1980 he gets a few roles in movies like Curse of the Pink Panther (1983) directed by Blake Edwards, and I Am the Cheese (1983).\\n\\n1990s \\nThe 1990s were very good for him getting the role of Number Two in Austin Powers (1997) with Mike Myers.\\nWagner returned to play the characters in Austin Powers: The Spy Who Shagged Me (1999), and Austin Powers in Goldmember (2002).\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does not cut sentences off in the middle\n",
    "chunked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the last decade, Wagner has worked in movies El padrino (2004), Hoot (2006), Man in the Chair (2007) with Christopher Plummer, The Wild Stallion (2009).\\n\\nTelevision \\nRobert Wagner starred in three successful series for American television. One of them was It Takes a Thief, Alexander Mundy a spy performing dangerous missions for the government of the United States. I also work in the series Fred Astaire as Alistair Mundy.\\n\\nWagner starred in the series for 66 episodes between 1968 and 1970. and was nominated for an Emmy and Golden Globe Awards in 1970\\n\\nIn 1975 Wagner stars with Eddie Albert, the series of detectives created by Glen A. Larson, Switch (1975 - 1978). he plays detective Pete T. Ryan.\\n\\nHis television series Hart to Hart (1979-1984), was most successful of all, Wagner is Jonathan Hart who with his wife Jennifer Hart, Stefanie Powers, two detectives who solved the most difficult criminal cases in high society.  \\n\\nThe series created by Sidney Sheldon, was an immediate success.\\n\\nMade several television movies including Hart to Hart, Hart to Hart Returns (1993), Hart to Hart: Home Is Where the Hart Is (1994), Hart to Hart: Crimes of the Hart (1994), Hart to Hart: Old Friends Never Die (1994), Hart to Hart: Two Harts in 3/4 Time (1995), Hart to Hart: Harts in High Season (1996) and Hart to Hart: Till Death Do Us Hart (1996).\\n\\nReferences\\n \\n \\n Robert Wagner on Yahoo! Movies\\n Articles about Robert Wagner, a Malibu resident, can be found at The Malibu Times \\n\\n1930 births\\nLiving people\\nActors from Detroit, Michigan\\nAmerican movie actors\\nAmerican television actors'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that there's no overlap here as the content starts in a new paragraph\n",
    "# If you change the chunk size above and try this, you'll see that overlaps \n",
    "# occur when content is chunked mid-paragraph\n",
    "chunked[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Chunking on Dataset\n",
    "\n",
    "Now we perform chunking on our entire dataset. We use the chunker's in-built function to do this on our list of documents. We get a list of [TextNodes](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_nodes/) as output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb90a7e235e743cfb61c4275936cb23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nodes = chunker.get_nodes_from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='fbe900c5-7ae6-4a65-a1a3-02e99913b46a', embedding=None, metadata={'title': 'Halal snack pack', 'url': 'https://simple.wikipedia.org/wiki/Halal%20snack%20pack'}, excluded_embed_metadata_keys=['url'], excluded_llm_metadata_keys=['url'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2d1934ad-ff3b-477e-bf75-ae53ffbc8609', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'Halal snack pack', 'url': 'https://simple.wikipedia.org/wiki/Halal%20snack%20pack'}, hash='18afa25a1df2aeb36bc851ab9b89d218845aabd0f35d1072b044b4c2f71d611b')}, text='A halal snack pack, or HSP, is a dish that comes from Australia. It is made up of halal-certified doner kebab meat (mainly lamb, chicken or beef), chips, sauces (mainly chili, garlic and barbecue sauces) and often cheese. The exact origin of the halal snack pack is unknown. Halal snack packs have become more popular since 2015, when the Facebook group \"Halal Snack Pack Appreciation Society\" was created.\\n\\nReferences\\n\\nAustralia\\nFast food', start_char_idx=0, end_char_idx=439, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "For our embedding model we use the [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) model. This is an MIT license model which is, at the time of writing, 44th on the [MTEB Retrieval leaderboard](https://huggingface.co/spaces/mteb/leaderboard). While there are better models available (as seen on the leaderboard), we choose this model for the demo as it's very small (33M parameters / ~120MB) and hence, very fast. The generated embeddings are 384-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashsaravanan/Library/Caches/pypoetry/virtualenvs/genai-bootcamp-4wh1UwyX-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", embed_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-6.65795133e-02,  8.12166780e-02, -4.37078811e-03, -3.80715914e-02,\n",
       "        3.39519978e-02,  9.77202877e-03,  5.77810593e-02, -1.30435536e-02,\n",
       "       -3.30868997e-02, -2.43122838e-02, -1.58980209e-02, -8.46297741e-02,\n",
       "       -7.33705889e-03,  4.63541150e-02,  3.02996412e-02, -3.61367203e-02,\n",
       "        3.10154110e-02, -6.56468794e-02, -2.36510281e-02, -2.89180316e-02,\n",
       "        2.44261045e-02,  7.21716508e-03, -6.03428483e-02, -6.56444952e-02,\n",
       "        6.78342879e-02,  1.44856526e-02,  2.76592355e-02,  2.35805195e-02,\n",
       "       -9.50826034e-02, -1.39449999e-01,  5.08472845e-02,  2.00165957e-02,\n",
       "        6.72311615e-03, -5.49212694e-02, -4.95373532e-02,  8.13274551e-03,\n",
       "        3.17828469e-02, -1.74102969e-02, -3.71145159e-02,  3.27568837e-02,\n",
       "        4.01726142e-02,  5.12109138e-02,  3.02632526e-02, -1.65336076e-02,\n",
       "        9.14637279e-03, -3.67910750e-02, -3.71675827e-02,  2.77871490e-02,\n",
       "        6.12801351e-02,  2.07614843e-02, -7.96210486e-03,  4.86738347e-02,\n",
       "       -2.19224058e-02, -1.11595402e-02,  3.52631770e-02, -1.32737011e-02,\n",
       "       -1.24294966e-05,  5.15412306e-03, -3.25361826e-02, -1.70886740e-02,\n",
       "        1.01281749e-02,  5.94114065e-02, -1.01311922e-01,  6.73841983e-02,\n",
       "       -5.39426766e-02,  7.17669120e-03, -2.04578545e-02,  1.73313208e-02,\n",
       "       -2.96410769e-02, -4.31972109e-02, -6.89444020e-02,  2.78859437e-02,\n",
       "        2.99694352e-02,  7.01792389e-02, -8.48121359e-04, -3.14916484e-02,\n",
       "        5.99448048e-02,  5.46079595e-03, -7.97196031e-02, -1.35090388e-02,\n",
       "       -9.32376832e-03,  1.16388751e-02,  3.24807735e-03, -1.07995551e-02,\n",
       "        7.89769983e-04, -6.14269972e-02,  6.21758178e-02, -3.74555774e-02,\n",
       "        4.23285663e-02, -4.23588678e-02, -9.74924304e-03, -1.97776426e-02,\n",
       "        1.82942320e-02, -6.13825917e-02, -9.43134502e-02,  4.20796871e-03,\n",
       "       -6.24038838e-02, -2.19229572e-02, -4.20769192e-02,  3.90921444e-01,\n",
       "       -7.97216222e-02,  3.92810926e-02,  2.81687528e-02,  6.30947668e-03,\n",
       "        7.51626305e-03, -2.55901776e-02,  1.07025560e-02,  6.63026944e-02,\n",
       "        1.37656024e-02, -1.18034482e-02, -3.19465296e-03, -1.13551309e-02,\n",
       "        3.67000736e-02, -2.70930165e-03, -1.46155534e-02, -6.26019984e-02,\n",
       "       -2.29455642e-02, -9.88182239e-03,  1.54108228e-02,  5.22992872e-02,\n",
       "       -5.58071993e-02,  3.66868488e-02,  1.03644557e-01, -1.28684780e-02,\n",
       "       -1.55532751e-02,  1.55406194e-02, -5.70918247e-02,  6.09402880e-02,\n",
       "        4.74069528e-02, -3.06361541e-02,  2.73186602e-02,  2.85820541e-04,\n",
       "        3.69200036e-02, -3.86619046e-02,  5.14680855e-02,  4.34415415e-03,\n",
       "        4.91905101e-02, -1.85506511e-03,  1.28719136e-02, -5.52858002e-02,\n",
       "        2.12458242e-02, -2.02608071e-02, -2.50680018e-02, -1.00776665e-01,\n",
       "       -5.34492917e-02, -5.31719737e-02, -9.82736610e-03,  2.94197127e-02,\n",
       "       -7.81114101e-02,  9.42963660e-02,  3.93285826e-02,  1.88866574e-02,\n",
       "        6.34389929e-03, -1.50882611e-02,  3.55504118e-02, -4.84831724e-03,\n",
       "        5.09219319e-02,  2.12116726e-02,  8.84336419e-03, -3.96665968e-02,\n",
       "       -7.72233354e-03, -9.36995745e-02, -9.81546845e-03,  2.59059053e-02,\n",
       "       -5.27734011e-02, -1.15184583e-01, -6.51037041e-03, -5.17417379e-02,\n",
       "        3.85927148e-02,  1.99643262e-02,  8.13918784e-02,  8.04470628e-02,\n",
       "       -1.18721155e-02, -2.46746186e-03,  1.26908123e-02,  1.14950091e-02,\n",
       "       -3.79238725e-02,  4.35097665e-02,  3.91408578e-02, -1.91399641e-02,\n",
       "        3.03715449e-02, -8.11552908e-03, -1.24009855e-01, -3.51900235e-02,\n",
       "        3.04868761e-02, -5.51962741e-02, -2.76491456e-02, -1.13762543e-02,\n",
       "        3.07628941e-02, -9.67028725e-04, -5.02066351e-02,  3.25357392e-02,\n",
       "       -5.33133037e-02, -7.32180523e-03,  1.07943295e-02, -1.43571738e-02,\n",
       "       -2.68685091e-02,  7.05144480e-02,  4.68679033e-02, -5.15261330e-02,\n",
       "        1.03883455e-02, -1.36085525e-02, -3.92644480e-02,  2.28888784e-02,\n",
       "        5.62093966e-02, -1.25151537e-02,  4.09756452e-02, -6.22224323e-02,\n",
       "        9.00993403e-03, -2.57668421e-02,  5.33512142e-03,  2.52762679e-02,\n",
       "       -3.81229408e-02,  4.70594801e-02, -1.84906442e-02, -4.29099659e-03,\n",
       "        8.38043392e-02,  2.19970550e-02, -1.33523764e-02, -4.04394930e-03,\n",
       "       -1.55390313e-04, -1.34746060e-01,  1.14828609e-02, -2.81281859e-01,\n",
       "        2.61119697e-02,  2.55843438e-03,  1.55408345e-02,  5.52468486e-02,\n",
       "        1.67168770e-02, -1.55459912e-02, -4.74315323e-02,  7.60248601e-02,\n",
       "        5.21599641e-03,  9.99256223e-02,  2.48453300e-02, -3.65541093e-02,\n",
       "        6.53757155e-02, -6.43654959e-03,  5.55126667e-02,  7.63012911e-04,\n",
       "       -2.12601293e-02,  1.07215848e-02,  5.90046346e-02, -3.69164236e-02,\n",
       "       -7.17076287e-03, -3.14803794e-02,  3.19188572e-02,  5.36245666e-02,\n",
       "       -3.72383893e-02,  1.32035881e-01,  7.75150582e-02,  4.64812815e-02,\n",
       "        4.58795764e-03,  1.06888369e-03,  6.99037462e-02,  9.14289989e-03,\n",
       "       -1.03879124e-01,  1.65590309e-02, -2.99867690e-02, -3.23159667e-03,\n",
       "       -2.73001995e-02,  2.87849531e-02,  4.28859796e-03, -4.30236869e-02,\n",
       "        1.10336754e-04, -5.44585697e-02, -1.03705846e-01,  3.73264365e-02,\n",
       "       -1.15921497e-02,  3.85447480e-02,  3.75192352e-02,  4.95472066e-02,\n",
       "        2.16242280e-02,  1.49514033e-02, -4.31099674e-03, -1.77282598e-02,\n",
       "        7.76751759e-03, -5.32355495e-02,  6.24663546e-04,  3.12939882e-02,\n",
       "       -2.39262786e-02, -4.76923846e-02, -1.98527686e-02, -3.32840234e-02,\n",
       "       -4.00570454e-03,  6.17818758e-02,  1.79786477e-02,  8.06940161e-03,\n",
       "        2.00051200e-02,  6.15084870e-03,  5.94644397e-02, -1.93987712e-02,\n",
       "        2.91878693e-02, -3.84448916e-02,  5.91026545e-02,  2.49495916e-03,\n",
       "       -1.89932734e-02,  2.62801964e-02,  4.18330431e-02,  7.36922817e-03,\n",
       "       -6.72366321e-02,  1.45708071e-02,  2.01329216e-02,  4.95241322e-02,\n",
       "        5.45808151e-02,  7.00030327e-02, -3.03261969e-02,  5.05630411e-02,\n",
       "        2.05216371e-02, -4.13991623e-02,  3.84434797e-02,  1.81411207e-02,\n",
       "        9.27000400e-03,  2.39336640e-02,  3.00820894e-03, -9.16593242e-04,\n",
       "        2.36354489e-02,  1.63559299e-02, -3.82680632e-02, -2.55613446e-01,\n",
       "        6.81011006e-02, -5.28698508e-03, -4.19134386e-02, -2.60237232e-02,\n",
       "        9.65674128e-03, -2.31260620e-02, -2.92329281e-03, -2.90806592e-02,\n",
       "       -1.97551120e-02,  2.71486603e-02,  2.43835784e-02,  5.95751218e-02,\n",
       "       -4.57267873e-02,  5.12397662e-02, -3.83423902e-02,  4.15022078e-04,\n",
       "        2.71884277e-02, -6.32092217e-03,  9.71386395e-03, -4.13079038e-02,\n",
       "       -2.56105643e-02,  1.28377751e-01,  6.02268800e-02, -2.45908648e-02,\n",
       "        1.55727859e-04,  2.91173160e-02, -5.34298681e-02, -2.06438806e-02,\n",
       "        5.85201085e-02,  5.24755493e-02, -4.99255061e-02,  3.13211083e-02,\n",
       "        2.04504468e-03,  1.31898718e-02,  6.21495917e-02,  2.22925469e-02,\n",
       "       -2.03879233e-02, -7.27250278e-02,  1.65927466e-02, -4.86844331e-02,\n",
       "        1.37904687e-02, -5.09304293e-02, -2.81790141e-02,  3.84076945e-02,\n",
       "       -6.51199743e-02, -3.34430188e-02, -3.12558599e-02, -1.68432444e-02,\n",
       "       -2.21869573e-02,  1.27138151e-02, -1.02112594e-03,  1.05077308e-02,\n",
       "       -1.85891259e-02, -5.09467162e-03, -1.22532826e-02, -1.64761655e-02,\n",
       "       -2.81150639e-02, -7.89874420e-02,  9.71406791e-03,  8.16036761e-02,\n",
       "        5.41116036e-02,  1.86175965e-02,  3.10841948e-02,  1.04209431e-01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of how to get an embedding and what it looks like\n",
    "print(np.array(embedding_model.get_text_embedding(nodes[0].text)).shape)\n",
    "np.array(embedding_model.get_text_embedding(nodes[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding & Indexing\n",
    "\n",
    "Llamaindex stores embeddings in a [VectorStoreIndex](https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/) object. This can be any used with any vector store [supported](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/) by Llamaindex. By default, this is a [SimpleIndex](https://docs.llamaindex.ai/en/stable/examples/vector_stores/SimpleIndexDemo/) which is a flat index. \n",
    "\n",
    "We can load all our chunks and embed them when creating a VectorStoreIndex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f90236db4cf40eab70a21a436125938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex(nodes, embedding_model=embedding_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "Now that we have a vector index, we can query this index. Internally, the query is converted into an embedding and a flat similarity search occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who is Robert Wagner?\n",
      "------------------------------------------------------------------------------------------\n",
      "Rank 1: Robert Wagner (0.9024011659794268)\n",
      "Robert John Wagner, Jr. (born February 10, 1930 in Detroit, Michigan, U.S.), is an American actor. H...\n",
      "------------------------------------------------------------------------------------------\n",
      "Rank 2: Robert Wagner (0.8440168825342981)\n",
      "In the last decade, Wagner has worked in movies El padrino (2004), Hoot (2006), Man in the Chair (20...\n",
      "------------------------------------------------------------------------------------------\n",
      "Rank 3: Hans Richter (0.8048237868714503)\n",
      "Hans Richter (born as Raab (now Györ) 4 April 1843; died Bayreuth 5 December 1916) was an Austro-Hun...\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Robert Wagner?\"\n",
    "results = index.as_retriever(similarity_top_k=3).retrieve(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"---\" * 30)\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Rank {i+1}: {result.metadata['title']} ({result.score})\")\n",
    "    print(result.text[:100] + \"...\")\n",
    "    print(\"---\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347 ms ± 77.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "results = index.as_retriever(similarity_top_k=3).retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG\n",
    "\n",
    "So now we have an index and a way to query the index. To close the RAG loop here, we need to get an input from a user, pass it to the retriever, send the retrieved context to the LLM, and return the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What movies did Robert Wagner make?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our prompt\n",
    "system_prompt = \"You are a helpful AI assistant that can answer questions about a wide range of topics. Use the given context to answer the user's question. If the information is not present in the context, you can say 'I don't know'. Otherwise, include the URL of the source you used.\"\n",
    "\n",
    "prompt = \"\"\"Context: \n",
    "{context} \n",
    "-----\n",
    "Question: {question} \n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Choice\n",
    "\n",
    "This demo supports two different APIs for models - OpenAI and AnyScale. Specifically, we use `gpt-4o` and `meta-llama/Meta-Llama-3-70B-Instruct` but any supported model should work. Note that AnyScale uses the same structure for API calls as OpenAI, just with a different url and API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(index, user_input):\n",
    "    results = index.as_retriever(similarity_top_k=3).retrieve(user_input)\n",
    "    return \"\\n----------------\\n\".join([result.metadata[\"title\"] + \"\\n\" + result.text + f\"\\nURL: {result.metadata['url']}\" for result in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert Wagner made numerous movies throughout his career. Some of the notable ones include:\n",
      "\n",
      "- The Happy Years (1950) (as an extra)\n",
      "- Halls of Montezuma (1951)\n",
      "- The Frogmen (1951)\n",
      "- What Price Glory? (1952)\n",
      "- Stars and Stripes Forever (1952)\n",
      "- Prince Valiant (1954)\n",
      "- Broken Lance (1954)\n",
      "- White Feather (1955)\n",
      "- A Kiss Before Dying (1956)\n",
      "- Between Heaven and Hell (1956)\n",
      "- The True Story of Jesse James (1957)\n",
      "- The Longest Day (1962)\n",
      "- The War Lover (1962)\n",
      "- The Pink Panther (1963)\n",
      "- Banning (1967)\n",
      "- The Towering Inferno (1974)\n",
      "- Midway (1976)\n",
      "- The Concorde... Airport '79 (1979)\n",
      "- Curse of the Pink Panther (1983)\n",
      "- I Am the Cheese (1983)\n",
      "- Austin Powers: International Man of Mystery (1997)\n",
      "- Austin Powers: The Spy Who Shagged Me (1999)\n",
      "- Austin Powers in Goldmember (2002)\n",
      "- El padrino (2004)\n",
      "- Hoot (2006)\n",
      "- Man in the Chair (2007)\n",
      "- The Wild Stallion (2009)\n",
      "\n",
      "For more details, you can visit the source: [Robert Wagner on Simple Wikipedia](https://simple.wikipedia.org/wiki/Robert%20Wagner).\n"
     ]
    }
   ],
   "source": [
    "# OpenAI\n",
    "from openai import OpenAI\n",
    "# Set up OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get('OPENAI_API_KEY'),\n",
    "    base_url = os.environ.get('OPENAI_BASE_URL')\n",
    ")\n",
    "\n",
    "# Run retrieval\n",
    "context = retrieve(index, user_input)\n",
    "\n",
    "# Get output\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "              {\"role\": \"user\", \"content\": prompt.format(context=context, question=user_input)}],\n",
    "    temperature=0.1\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, Robert Wagner made the following movies:\n",
      "\n",
      "1. The Happy Years (1950)\n",
      "2. Halls of Montezuma (1951)\n",
      "3. The Frogmen (1951)\n",
      "4. What Price Glory? (1952)\n",
      "5. Stars and Stripes Forever (1952)\n",
      "6. Prince Valiant (1954)\n",
      "7. Broken Lance (1954)\n",
      "8. White Feather (1955)\n",
      "9. A Kiss Before Dying (1956)\n",
      "10. Between Heaven and Hell (1956)\n",
      "11. The True Story of Jesse James (1957)\n",
      "12. The Longest Day (1962)\n",
      "13. The War Lover (1962)\n",
      "14. The Pink Panther (1963)\n",
      "15. Banning (1967)\n",
      "16. The Towering Inferno (1974)\n",
      "17. Midway (1976)\n",
      "18. The Concorde... Airport '79 (1979)\n",
      "19. Curse of the Pink Panther (1983)\n",
      "20. I Am the Cheese (1983)\n",
      "21. Austin Powers (1997)\n",
      "22. Austin Powers: The Spy Who Shagged Me (1999)\n",
      "23. Austin Powers in Goldmember (2002)\n",
      "24. El padrino (2004)\n",
      "25. Hoot (2006)\n",
      "26. Man in the Chair (2007)\n",
      "27. The Wild Stallion (2009)\n",
      "\n",
      "Source: https://simple.wikipedia.org/wiki/Robert%20Wagner\n"
     ]
    }
   ],
   "source": [
    "# AnyScale\n",
    "from openai import OpenAI\n",
    "# Set up client\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get('ANYSCALE_API_KEY'),\n",
    "    base_url = os.environ.get('ANYSCALE_BASE_URL')\n",
    ")\n",
    "\n",
    "# Run retrieval\n",
    "context = retrieve(index, user_input)\n",
    "\n",
    "# Get output\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "              {\"role\": \"user\", \"content\": prompt.format(context=context, question=user_input)}],\n",
    "    temperature=0.1\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps: Using a Vectorstore (Vector DB)\n",
    "\n",
    "We use LanceDB instead of the default index used above. As described [here](https://lancedb.github.io/lancedb/ann_indexes/), LanceDB uses a disk-based IVF-PQ index. As they note in the same page, this is usually only necessary when you have 100k+ samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/examples/vector_stores/LanceDBIndexDemo/\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# Create your DB locally\n",
    "vector_store = LanceDBVectorStore(\n",
    "    uri=\"./lancedb\", table_name=\"test\"\n",
    ")\n",
    "# Link to the collection on llamaindex\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311bb247e902448d82e9426dcfcf7fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-03T20:33:48Z WARN  lance::dataset] No existing dataset at /Users/akashsaravanan/Downloads/GenAI Bootcamp/genai-bootcamp/notebooks/lancedb/test.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "# Embed and index\n",
    "index = VectorStoreIndex(nodes, embed_model=embedding_model, storage_context=storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the index from disk\n",
    "vector_store = LanceDBVectorStore(\n",
    "    uri=\"./lancedb\", table_name=\"test\"\n",
    ")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who is Robert Wagner?\n",
      "------------------------------------------------------------------------------------------\n",
      "Rank 1: Robert Wagner (0.6897997260093689)\n",
      "Robert John Wagner, Jr. (born February 10, 1930 in Detroit, Michigan, U.S.), is an American actor. H...\n",
      "------------------------------------------------------------------------------------------\n",
      "Rank 2: Robert Wagner (0.6023688316345215)\n",
      "In the last decade, Wagner has worked in movies El padrino (2004), Hoot (2006), Man in the Chair (20...\n",
      "------------------------------------------------------------------------------------------\n",
      "Rank 3: Hans Richter (0.46540871262550354)\n",
      "Hans Richter (born as Raab (now Györ) 4 April 1843; died Bayreuth 5 December 1916) was an Austro-Hun...\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Robert Wagner?\"\n",
    "results = index.as_retriever(similarity_top_k=3).retrieve(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"---\" * 30)\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Rank {i+1}: {result.metadata['title']} ({result.score})\")\n",
    "    print(result.text[:100] + \"...\")\n",
    "    print(\"---\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.7 ms ± 2.98 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "results = index.as_retriever(similarity_top_k=3).retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps: Ingestion Pipeline\n",
    "\n",
    "While we use a low level API above to execute each step individuall, we can also build this in the form of a pipeline. For the sake of completeness, we repeat all steps from the start including the data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data again\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"wikipedia\", \"20220301.simple\", trust_remote_code=True)\n",
    "\n",
    "sample_size = 1000\n",
    "np.random.seed(42)\n",
    "data = data[\"train\"][np.random.choice(data[\"train\"].shape[0], size=sample_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 22805.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# We load our dataset into a list of llamaindex documents\n",
    "from llama_index.core import Document\n",
    "\n",
    "documents = []\n",
    "for i in tqdm.tqdm(range(len(data[\"text\"]))):\n",
    "    documents.append(\n",
    "        Document(\n",
    "            text=data[\"text\"][i],\n",
    "            metadata={\"title\": data[\"title\"][i], \"url\": data[\"url\"][i]},\n",
    "            excluded_embed_metadata_keys=[\"url\"], # We don't embed the url\n",
    "            excluded_llm_metadata_keys=[\"url\"], # We don't send the url to LLM\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/examples/vector_stores/LanceDBIndexDemo/\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# Create your DB locally\n",
    "vector_store = LanceDBVectorStore(\n",
    "    uri=\"./lancedb\", table_name=\"pipeline_test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashsaravanan/Library/Caches/pypoetry/virtualenvs/genai-bootcamp-4wh1UwyX-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
    "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", embed_batch_size=32),\n",
    "    ],\n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7fbb08579b483981b3612faaf904c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d203cc7b33e4324bc137005cfbb4161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-03T20:41:39Z WARN  lance::dataset] No existing dataset at /Users/akashsaravanan/Downloads/GenAI Bootcamp/genai-bootcamp/notebooks/lancedb/pipeline_test.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "nodes = pipeline.run(documents=documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex(nodes, embed_model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who is Robert Wagner?\n",
      "------------------------------------------------------------------------------------------\n",
      "Rank 1: Robert Wagner (0.814323011795492)\n",
      "Robert John Wagner, Jr. (born February 10, 1930 in Detroit, Michigan, U.S.), is an American actor. H...\n",
      "------------------------------------------------------------------------------------------\n",
      "Rank 2: Robert Wagner (0.7465573569177086)\n",
      "In the last decade, Wagner has worked in movies El padrino (2004), Hoot (2006), Man in the Chair (20...\n",
      "------------------------------------------------------------------------------------------\n",
      "Rank 3: Hans Richter (0.6175803556028695)\n",
      "Hans Richter (born as Raab (now Györ) 4 April 1843; died Bayreuth 5 December 1916) was an Austro-Hun...\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Robert Wagner?\"\n",
    "results = index.as_retriever(similarity_top_k=3).retrieve(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"---\" * 30)\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Rank {i+1}: {result.metadata['title']} ({result.score})\")\n",
    "    print(result.text[:100] + \"...\")\n",
    "    print(\"---\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-bootcamp-4wh1UwyX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
