{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Next Steps: Deploying Tuned model on SageMaker "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need an AWS account to run this tutorial. Let's start by importing the AWS keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env AWS_ACCESS_KEY_ID=******\n",
    "# %env AWS_SECRET_ACCESS_KEY=******\n",
    "# %env AWS_DEFAULT_REGION=us-east-1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy an existing Hugging Face model, such as Falcon 40B, you can follow this tutorial [Deploy Falcon 7B & 40B on Amazon SageMaker](https://www.philschmid.de/sagemaker-falcon-llm). But today we are going to deploy our own tuned model with LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's install deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "!pip install -q git+https://github.com/huggingface/peft\n",
    "!pip install -q datasets==2.12.0 evaluate==0.4.0 numpy==1.24.3 torch==2.0.1 tqdm==4.65.0 transformers==4.29.2 ipykernel ipywidgets sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AdamW, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from sagemaker.huggingface.model import HuggingFaceModel, HuggingFacePredictor\n",
    "from peft import get_peft_model, TaskType, LoraConfig\n",
    "import functools\n",
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "import evaluate\n",
    "import time\n",
    "import tarfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LoRA model with Trainer. \n",
    "For more details look at [Day 2: Next Steps: Finetuning](https://github.com/georgian-io/genai-bootcamp/blob/main/notebooks/day-2/06-fine-tuning.ipynb). \n",
    "You can use your own checkpoint as an alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset financial_phrasebank (/home/ubuntu/.cache/huggingface/datasets/financial_phrasebank/sentences_allagree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a79bbe05672412e97eab975aa5116cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/ubuntu/.cache/huggingface/datasets/financial_phrasebank/sentences_allagree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141/cache-70d5ee610c33551c.arrow and /home/ubuntu/.cache/huggingface/datasets/financial_phrasebank/sentences_allagree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141/cache-d30fb0cf1bd2beb4.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/financial_phrasebank/sentences_allagree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141/cache-f788c6abf00c795b.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/financial_phrasebank/sentences_allagree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141/cache-a5b1561fc5e1eba2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 784,329,728 || trainable%: 0.15040205131686657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args = Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=True,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0003,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=model_for_sagemaker/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=epoch,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=model_for_sagemaker,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=model_for_sagemaker,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"google/flan-t5-large\" # https://huggingface.co/google/flan-t5-large\n",
    "peft_model_id = \"model_for_sagemaker\"\n",
    "\n",
    "\n",
    "r = 4 # LoRA attention dimension parameter\n",
    "dropout_rate = 0.1\n",
    "batch_size = 8\n",
    "n_epochs = 3\n",
    "lr = 3e-4\n",
    "max_length = 128\n",
    "grad_accumulation_steps = 1\n",
    "\n",
    " \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def preprocess_function(examples, tokenizer, text_column, label_column, max_length):\n",
    "    inputs = [text.replace('\\n', ' ') for text in examples[text_column]]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    targets = examples[\"text_label\"]\n",
    "    labels = tokenizer(targets, max_length=2, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = labels[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    model_inputs[\"labels\"] = labels\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# load the dataset\n",
    "# sentences_allagree means all annotators agreed on the label\n",
    "dataset = datasets.load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "text_column = \"sentence\"\n",
    "label_column = \"label\"\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=200, seed=42, shuffle=True)\n",
    "dataset[\"validation\"] = dataset[\"test\"]\n",
    "del dataset[\"test\"]\n",
    "\n",
    "classes = dataset[\"train\"].features[label_column].names\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[label_column]]},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "\n",
    "# Build Model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Use LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=r, lora_alpha=32, lora_dropout=dropout_rate\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Preprocess dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    functools.partial(preprocess_function, tokenizer=tokenizer, text_column=text_column, label_column=label_column, max_length=max_length),\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"validation\"]\n",
    "\n",
    "label_pad_token_id = -100\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, label_pad_token_id=label_pad_token_id, pad_to_multiple_of=8\n",
    ")\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accumulation_steps,\n",
    "    output_dir=peft_model_id,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=n_epochs,\n",
    "    logging_dir=f\"{peft_model_id}/logs\",\n",
    ")\n",
    "\n",
    "print(f\"training_args = {training_args}\")\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/vn/genai-bootcamp/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='774' max='774' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [774/774 03:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.607600</td>\n",
       "      <td>0.075982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.041550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.054657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.054656725376844406,\n",
       " 'eval_runtime': 2.1386,\n",
       " 'eval_samples_per_second': 93.518,\n",
       " 'eval_steps_per_second': 11.69,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "trainer.model.base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t     checkpoint-516\t     pytorch_model.bin\r\n",
      "adapter_config.json  checkpoint-774\t     special_tokens_map.json\r\n",
      "adapter_model.bin    config.json\t     tokenizer.json\r\n",
      "checkpoint-258\t     generation_config.json  tokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls {peft_model_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packaging model for inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use [sagemaker-huggingface-inference-toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit/) for infernce. To customize libraries and the inference function, we need to define custom inference.py and requirements.txt files.\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define infenrece.py with custom model_fn to load LoRA model and predict_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "MAX_NEW_TOKEN = 256\n",
    "TEMPERATURE = 0.001\n",
    "\n",
    "\n",
    "def model_fn(model_dir: str) -> Tuple[PeftModel, AutoTokenizer]:\n",
    "    peft_model_id = model_dir\n",
    "    config = PeftConfig.from_pretrained(peft_model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "    model_lora = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "    model_lora = PeftModel.from_pretrained(model_lora, peft_model_id)\n",
    "    model_lora = model_lora.merge_and_unload()\n",
    "    model_lora.eval()\n",
    "    model_lora.cuda()\n",
    "    return model_lora, tokenizer\n",
    "\n",
    "\n",
    "def predict_fn(data: Dict[str, str], model_and_tokenizer: Tuple[PeftModel, AutoTokenizer]):\n",
    "    # destruct model and tokenizer\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "\n",
    "    # Tokenize sentences\n",
    "    prompt = data.pop(\"prompt\", data)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=input_ids, max_new_tokens=MAX_NEW_TOKEN, temperature=TEMPERATURE)\n",
    "    print(f\"input prompt: {prompt}\\n{'---'* 20}\")\n",
    "    result = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
    "    print(f\"model output:\\n{result}\")\n",
    "\n",
    "    return {\"result\": result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "transformers==4.29.1\n",
    "peft==0.3.0\n",
    "bitsandbytes==0.38.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we need to create a model.tar.gz archive for your model file and code using the package_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_model(path_to_model_folder: Path = Path(\"\"), path_to_result: Path = Path(\"model.tar.gz\")) -> None:\n",
    "    path_to_model_folder = Path(path_to_model_folder)\n",
    "    path_to_result = Path(path_to_result)\n",
    "\n",
    "    # Check if path_to_model_folder is a directory\n",
    "    if not path_to_model_folder.is_dir():\n",
    "        raise NotADirectoryError(f\"{path_to_model_folder} is not a directory\")\n",
    "\n",
    "    # Ensure the result path is a tar.gz file\n",
    "    if not str(path_to_result).endswith(\".tar.gz\"):\n",
    "        raise ValueError(f\"{path_to_result} should end with .tar.gz\")\n",
    "\n",
    "    list_files_to_save = [\n",
    "        \"adapter_config.json\",\n",
    "        \"adapter_model.bin\",\n",
    "        \"config.json\",\n",
    "        \"generation_config.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"pytorch_model.bin\",\n",
    "    ]\n",
    "    with tarfile.open(path_to_result, \"w:gz\") as tar:\n",
    "        print(\"Adding code folder with inference and requirements.txt.\")\n",
    "        code_foler = path_to_model_folder / \"code\"\n",
    "        code_foler.mkdir(exist_ok=True)\n",
    "        shutil.copy(\"requirements.txt\", code_foler / \"requirements.txt\")\n",
    "        shutil.copy(\"inference.py\", code_foler / \"inference.py\")\n",
    "        tar.add(code_foler, arcname=\"code\")\n",
    "\n",
    "        print(f\"Adding {list_files_to_save} model files to model tar.gz.\")\n",
    "        for file_name in list_files_to_save:\n",
    "            tar.add(path_to_model_folder / file_name, arcname=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding code folder with inference and requirements.txt.\n",
      "Adding ['adapter_config.json', 'adapter_model.bin', 'config.json', 'generation_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'pytorch_model.bin'] model files to model tar.gz.\n"
     ]
    }
   ],
   "source": [
    "package_model(path_to_model_folder=peft_model_id, path_to_result=Path(\"model.tar.gz\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8G\tmodel.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh model.tar.gz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload this tar file to S3, where SageMaker can find it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DATA_S3_PATH = \"s3://genai-bootcamp/test-example/model.tar.gz\"\n",
    "ROLE = \"arn:aws:iam::823217009914:role/service-role/AmazonSageMaker-ExecutionRole-20180305T161813\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://genai-bootcamp/test-example/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp model.tar.gz {MODEL_DATA_S3_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy model: Create model, endpoint configuration and endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.huggingface.model.HuggingFacePredictor at 0x7feb765938b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = \"genai-bootcamp-flan\"\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=MODEL_DATA_S3_PATH,\n",
    "    role=ROLE,\n",
    "    name=endpoint_name,\n",
    "    transformers_version=\"4.26\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    py_version=\"py39\",\n",
    ")\n",
    "\n",
    "huggingface_model.deploy(initial_instance_count=1, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  genai-bootcamp-flan                                              |\r\n"
     ]
    }
   ],
   "source": [
    "!aws sagemaker list-models --query 'Models[].ModelName' --output table | grep genai-bootcamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||  EndpointArn     |  arn:aws:sagemaker:us-east-1:823217009914:endpoint/genai-bootcamp-flan-2023-06-18-22-57-23-846   ||\r\n",
      "||  EndpointName    |  genai-bootcamp-flan-2023-06-18-22-57-23-846                                                     ||\r\n"
     ]
    }
   ],
   "source": [
    "!aws sagemaker list-endpoints --output table | grep genai-bootcamp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response = {'result': 'neutral'}; time = 1.5374922808259726\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictor = HuggingFacePredictor(\"genai-bootcamp-flan-2023-06-18-22-57-23-846\")\n",
    "s = time.monotonic()\n",
    "prompt = \"test\"\n",
    "response = predictor.predict({\"prompt\": prompt})\n",
    "e = time.monotonic()\n",
    "print(f\"response = {response}; time = {e - s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (ValidationException) when calling the DeleteModel operation: Could not find model \"arn:aws:sagemaker:us-east-1:823217009914:model/genai-bootcamp-flan\".\n",
      "\n",
      "An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:us-east-1:823217009914:endpoint/genai-bootcamp-flan-2023-06-18-22-57-23-846\".\n"
     ]
    }
   ],
   "source": [
    "!aws sagemaker delete-model --model-name {endpoint_name}\n",
    "!aws sagemaker delete-endpoint-config --endpoint-config-name genai-bootcamp-flan-2023-06-18-22-57-23-846\n",
    "!aws sagemaker delete-endpoint --endpoint-name genai-bootcamp-flan-2023-06-18-22-57-23-846"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
