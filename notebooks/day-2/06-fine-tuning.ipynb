{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Next Steps: Finetuning\n",
    "\n",
    "Let's walk through the finetuning process step by step for a classification task using LoRA.\n",
    "\n",
    "We also have a more full fledged finetuning script in the `finetuning` folder which lets you easily run 4 different types of PEFT with a single script.\n",
    "\n",
    "NOTE: Your local machine likely does not have the memory required (at least 24 GB on the GPU) to run this notebook. We thus recommend running this on an instance such as an AWS `g5.4xlarge` instance or a GCP instance with an A100 40GB GPU. \n",
    "\n",
    "NOTE 2: While this notebook may seem pretty long, the majority of it follows standard practices for training ML models. The key bit of code here is the following:\n",
    "```\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=r, lora_alpha=32, lora_dropout=dropout_rate\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "```\n",
    "\n",
    "NOTE 3: If you want to play around with other types of finetuning, check out the scripts in the `finetuning` folder!\n",
    "\n",
    "\n",
    "This creates a PEFT configuration (LoRA here) and we use it to get a version of our model using LoRA. Read on through this notebook to see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "!pip install git+https://github.com/huggingface/peft\n",
    "!pip install datasets==2.12.0 evaluate==0.4.0 numpy==1.24.3 torch==2.0.1 tqdm==4.65.0 transformers==4.29.2 ipykernel ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AdamW\n",
    "from peft import get_peft_model, TaskType, LoraConfig\n",
    "import functools\n",
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Lets set up some parameters for our model. In this case, the model is Google's Flan-T5-Large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"google/flan-t5-large\" # https://huggingface.co/google/flan-t5-large\n",
    "\n",
    "r = 4 # LoRA attention dimension parameter\n",
    "dropout_rate = 0.1\n",
    "batch_size = 8\n",
    "n_epochs = 3\n",
    "lr = 3e-4\n",
    "max_length = 128\n",
    "grad_accumulation_steps = 1\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We use the [Financial Phrasebank dataset](https://huggingface.co/datasets/financial_phrasebank/viewer/sentences_allagree/train) which contains sentiments about financial news.\n",
    "\n",
    "We first define a simple pre-processing function for our dataset. It just replaces new lines with a blank space, tokenizes the inputs and prepares the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, text_column, label_column, max_length):\n",
    "    inputs = [text.replace('\\n', ' ') for text in examples[text_column]]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    targets = examples[\"text_label\"]\n",
    "    labels = tokenizer(targets, max_length=2, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = labels[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    model_inputs[\"labels\"] = labels\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load our dataset, and prepare our training and validation data. For the purposes of this demo, we just split 200 examples from the training set to use for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset financial_phrasebank (/home/akash_saravanan_georgian_io/.cache/huggingface/datasets/financial_phrasebank/sentences_allagree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n",
      "100%|██████████| 1/1 [00:00<00:00, 718.57it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /home/akash_saravanan_georgian_io/.cache/huggingface/datasets/financial_phrasebank/sentences_allagree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141/cache-70d5ee610c33551c.arrow and /home/akash_saravanan_georgian_io/.cache/huggingface/datasets/financial_phrasebank/sentences_allagree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141/cache-d30fb0cf1bd2beb4.arrow\n",
      "                                                    \r"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "# sentences_allagree means all annotators agreed on the label\n",
    "dataset = datasets.load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "text_column = \"sentence\"\n",
    "label_column = \"label\"\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=200, seed=42, shuffle=True)\n",
    "dataset[\"validation\"] = dataset[\"test\"]\n",
    "del dataset[\"test\"]\n",
    "\n",
    "classes = dataset[\"train\"].features[label_column].names\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[label_column]]},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Next, lets load our model and apply PEFT, specifically LoRA to it! This is a simple as creating a peft configuration, such as `peft.LoraConfig` and passing it to the `get_peft_model` function alongside your model! Note that PEFT is a relatively new technique and as such both the supported models and the supported techniques can and will change over time. Please refer to the [HuggingFace documentation](https://huggingface.co/docs/peft/main/en/index) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 784,329,728 || trainable%: 0.15040205131686657\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Use LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=r, lora_alpha=32, lora_dropout=dropout_rate\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, we've dropped our model down from 784M parameters to just 1.1M parameters that need to be trained! The rest of this notebook follows a standard training procedure. \n",
    "\n",
    "First we pre-process our data and build the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    }
   ],
   "source": [
    "# Preprocess dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    functools.partial(preprocess_function, tokenizer=tokenizer, text_column=text_column, label_column=label_column, max_length=max_length),\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"validation\"]\n",
    "\n",
    "# Build dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=8, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/finetuning/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 258/258 [01:11<00:00,  3.62it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(1.8698, device='cuda:0') train_epoch_loss=tensor(0.6258, device='cuda:0') eval_ppl=tensor(1.0450, device='cuda:0') eval_epoch_loss=tensor(0.0440, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 258/258 [01:11<00:00,  3.60it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=tensor(1.0452, device='cuda:0') train_epoch_loss=tensor(0.0442, device='cuda:0') eval_ppl=tensor(1.0656, device='cuda:0') eval_epoch_loss=tensor(0.0635, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 258/258 [01:11<00:00,  3.60it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl=tensor(1.0269, device='cuda:0') train_epoch_loss=tensor(0.0265, device='cuda:0') eval_ppl=tensor(1.0434, device='cuda:0') eval_epoch_loss=tensor(0.0425, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model training loop\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step % grad_accumulation_steps == 0) or step == len(train_dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        eval_preds.extend(preds)\n",
    "    # Calculate metrics\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate using the accuracy on the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=97.0 % on the evaluation dataset\n",
      "eval_preds[40:60]=['neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'negative', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral']\n",
      "dataset['validation']['text_label'][40:60]=['neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'negative', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for pred, true in zip(eval_preds, dataset[\"validation\"][\"text_label\"]):\n",
    "    if pred.strip() == true.strip():\n",
    "        correct += 1\n",
    "    total += 1\n",
    "accuracy = correct / total * 100\n",
    "print(f\"{accuracy=} % on the evaluation dataset\")\n",
    "print(f\"{eval_preds[40:60]=}\")\n",
    "print(f\"{dataset['validation']['text_label'][40:60]=}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
