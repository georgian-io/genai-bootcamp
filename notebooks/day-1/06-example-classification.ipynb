{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 - Examples: Classification\n",
    "\n",
    "In this notebook we offer an example of classification via prompting. That is, given some text, we ask the model to classify the text into things like topics or sentiment. \n",
    "\n",
    "References/Further Reading:\n",
    "\n",
    "* This tutorial is partially based on https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/text_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup template\n",
    "\n",
    "We setup a template here telling the model to classify news headlines into one of 5 categories. Notice that we have provided the model with a number of examples of what the output should look like. This is called *few-shot prompting*! This helps the model to \"understand\" the kind of answer that we want from it.\n",
    "\n",
    "Feel free to modify this to your own needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classify the given news headlines into one of the following categories: [business, entertainment, health, sports, technology]\n",
      "\n",
      "Text: Pixel 7 Pro Expert Hands On Review. \n",
      "The answer is: technology \n",
      "\n",
      "Text: Quit smoking? \n",
      "The answer is: health \n",
      "\n",
      "Text: Birdies or bogeys? Top 5 tips to hit under par \n",
      "The answer is: sports \n",
      "\n",
      "Text: Relief from local minimum-wage hike looking more remote \n",
      "The answer is: business \n",
      "\n",
      "Text: <article> \n",
      "The answer is: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_article = \"<article>\"\n",
    "template_prompt = f\"\"\"\n",
    "Classify the given news headlines into one of the following categories: [business, entertainment, health, sports, technology]\n",
    "\n",
    "Text: Pixel 7 Pro Expert Hands On Review. \n",
    "The answer is: technology \n",
    "\n",
    "Text: Quit smoking? \n",
    "The answer is: health \n",
    "\n",
    "Text: Birdies or bogeys? Top 5 tips to hit under par \n",
    "The answer is: sports \n",
    "\n",
    "Text: Relief from local minimum-wage hike looking more remote \n",
    "The answer is: business \n",
    "\n",
    "Text: {selected_article} \n",
    "The answer is: \n",
    "\"\"\"\n",
    "\n",
    "print(template_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Input\n",
    "\n",
    "Here we add in the document we want the model to read. Feel free to modify this to a document of your choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_article = \"Introducing Apple Vision Pro: Appleâ€™s first spatial computer\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "As you might expect by now, it's just a matter of sending your prompt to the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technology\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "chatbot_response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=messages,\n",
    "  temperature=1,\n",
    "  max_tokens=1500,\n",
    ")\n",
    "\n",
    "print(chatbot_response.choices[0].message[\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "\n",
    "You know the drill now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technology\n"
     ]
    }
   ],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "response = model.predict(prompt, max_output_tokens=1024)\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source LLM: Falcon-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Technology\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ.get('HUGGINGFACEHUB_API_TOKEN')}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.request(\"POST\", os.environ.get(\"HUGGINGFACEHUB_ENDPOINT\"), headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "data = query({\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 10, \"return_full_text\": False, \"top_k\": 10, \"temperature\": 1}})\n",
    "\n",
    "print(data[0]['generated_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous examples, we had to fiddle with the parameters to get Falcon to work well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "Let's consider sentiment classification as another example. Given some text, we need to determine if the text is positive, negative or neutral."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup template\n",
    "\n",
    "This time we setup a zero-shot prompting example to classify review sentiment.\n",
    "\n",
    "Feel free to modify this to your own needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
      "\n",
      "Text: <article> \n",
      "The answer is: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_article = \"<article>\"\n",
    "template_prompt = f\"\"\"Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
    "\n",
    "Text: {selected_article} \n",
    "The answer is: \n",
    "\"\"\"\n",
    "\n",
    "print(template_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Input\n",
    "\n",
    "Here we add in the document we want the model to read. Feel free to modify this to a document of your choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_article = \"I loved the new Spider-Man movie!! The animation was really fluid\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "chatbot_response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=messages,\n",
    "  temperature=1,\n",
    "  max_tokens=1500,\n",
    ")\n",
    "\n",
    "print(chatbot_response.choices[0].message[\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "response = model.predict(prompt, max_output_tokens=1024)\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source LLM: Falcon-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ.get('HUGGINGFACEHUB_API_TOKEN')}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.request(\"POST\", os.environ.get(\"HUGGINGFACEHUB_ENDPOINT\"), headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "data = query({\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 250, \"return_full_text\": False}})\n",
    "\n",
    "print(data[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-bootcamp-4wh1UwyX-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
