{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 - Examples: Classification\n",
    "\n",
    "In this notebook we offer an example of classification via prompting. That is, given some text, we ask the model to classify the text into things like topics or sentiment. \n",
    "\n",
    "References/Further Reading:\n",
    "\n",
    "* This tutorial is partially based on https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/text_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup template\n",
    "\n",
    "We setup a template here telling the model to classify news headlines into one of 5 categories. Notice that we have provided the model with a number of examples of what the output should look like. This is called *few-shot prompting*! This helps the model to \"understand\" the kind of answer that we want from it.\n",
    "\n",
    "Later in this notebook, when we cover sentiment analysis, we perform *zero-shot prompting*. That is, we do not give the model an example of what it should do. We simply ask it for an answer directly. In-between zero-shot and few-shot prompting lies *one-shot prompting* where we give the model a single example. We illustrate this further towards the end of the notebook where we cover a concept known as Chain of Thought (CoT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classify the given news headlines into one of the following categories: [business, entertainment, health, sports, technology]\n",
      "\n",
      "Text: Pixel 7 Pro Expert Hands On Review. \n",
      "The answer is: technology \n",
      "\n",
      "Text: Quit smoking? \n",
      "The answer is: health \n",
      "\n",
      "Text: Birdies or bogeys? Top 5 tips to hit under par \n",
      "The answer is: sports \n",
      "\n",
      "Text: Relief from local minimum-wage hike looking more remote \n",
      "The answer is: business \n",
      "\n",
      "Text: <article> \n",
      "The answer is: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_article = \"<article>\"\n",
    "template_prompt = f\"\"\"\n",
    "Classify the given news headlines into one of the following categories: [business, entertainment, health, sports, technology]\n",
    "\n",
    "Text: Pixel 7 Pro Expert Hands On Review. \n",
    "The answer is: technology \n",
    "\n",
    "Text: Quit smoking? \n",
    "The answer is: health \n",
    "\n",
    "Text: Birdies or bogeys? Top 5 tips to hit under par \n",
    "The answer is: sports \n",
    "\n",
    "Text: Relief from local minimum-wage hike looking more remote \n",
    "The answer is: business \n",
    "\n",
    "Text: {selected_article} \n",
    "The answer is: \n",
    "\"\"\"\n",
    "\n",
    "print(template_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Input\n",
    "\n",
    "Here we add in the document we want the model to read. Feel free to modify this to a document of your choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_article = \"Introducing Apple Vision Pro: Appleâ€™s first spatial computer\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "As you might expect by now, it's just a matter of sending your prompt to the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technology\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "chatbot_response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=messages,\n",
    "  temperature=1,\n",
    "  max_tokens=1500,\n",
    ")\n",
    "\n",
    "print(chatbot_response.choices[0].message[\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "\n",
    "You know the drill now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technology\n"
     ]
    }
   ],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "response = model.predict(prompt, max_output_tokens=1024)\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source LLM: Falcon-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Technology\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ.get('HUGGINGFACEHUB_API_TOKEN')}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.request(\"POST\", os.environ.get(\"HUGGINGFACEHUB_ENDPOINT\"), headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "data = query({\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 10, \"return_full_text\": False, \"top_k\": 10, \"temperature\": 1}})\n",
    "\n",
    "print(data[0]['generated_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we had to fiddle with the parameters to get Falcon to work well. This is mostly just because Falcon-7B is a much smaller model than the other two. A larger open source model such as Falcon-40B would do a lot better!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "Let's consider sentiment classification as another example. Given some text, we need to determine if the text is positive, negative or neutral. As mentioned above, we follow a *zero-shot prompting* technique here, so we don't give the model prior examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup template\n",
    "\n",
    "This time we setup a zero-shot prompting example to classify review sentiment.\n",
    "\n",
    "Feel free to modify this to your own needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
      "\n",
      "Text: <article> \n",
      "The answer is: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_article = \"<article>\"\n",
    "template_prompt = f\"\"\"Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
    "\n",
    "Text: {selected_article} \n",
    "The answer is: \n",
    "\"\"\"\n",
    "\n",
    "print(template_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Input\n",
    "\n",
    "Here we add in the document we want the model to read. Feel free to modify this to a document of your choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_article = \"I loved the new Spider-Man movie!! The animation was really fluid\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "chatbot_response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=messages,\n",
    "  temperature=1,\n",
    "  max_tokens=1500,\n",
    ")\n",
    "\n",
    "print(chatbot_response.choices[0].message[\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "response = model.predict(prompt, max_output_tokens=1024)\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source LLM: Falcon-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ.get('HUGGINGFACEHUB_API_TOKEN')}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.request(\"POST\", os.environ.get(\"HUGGINGFACEHUB_ENDPOINT\"), headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "data = query({\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 250, \"return_full_text\": False}})\n",
    "\n",
    "print(data[0]['generated_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought (CoT) Classification\n",
    "\n",
    "Chain of Thought is a prompting technique that can help to improve a model's output by forcing it to reason about its actions. In essence, it allows us to \"see\" the model's thought process. You can read more about Chain of Thought [here](https://www.promptingguide.ai/techniques/cot).\n",
    "\n",
    "Lets perform CoT reasoning on the same sentiment analysis problem from above. Note that we only show this with a single API, but given that only the prompt changes, you should be able to use it with any of the other APIs!\n",
    "\n",
    "### First, lets find a situation where the model fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
      "\n",
      "Text: <article> \n",
      "The answer is: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_article = \"<article>\"\n",
    "template_prompt = f\"\"\"Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
    "\n",
    "Text: {selected_article} \n",
    "The answer is: \n",
    "\"\"\"\n",
    "\n",
    "print(template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_article = \"I really loved the new Spider-Man movie. The sub-par animation really made it amazing!\"\n",
    "# Note: This is a lie. The movie is actually fantastic and highly recommended!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "chatbot_response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=messages,\n",
    "  temperature=1,\n",
    "  max_tokens=1500,\n",
    ")\n",
    "\n",
    "print(chatbot_response.choices[0].message[\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to have tricked our model into thinking we loved the new Spider-Man movie with a little bit of sarcasm. \n",
    "\n",
    "### Now, let's try to fix this using CoT reasoning!\n",
    "\n",
    "Notice that we provide a single example (one-shot prompting) and explain our reasoning in the answer there (chain of thought)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
      "\n",
      "Text: My new pillow is so soft and fluffy! I hate it.\n",
      "The answer is: The reviewer initially praised the pillow but then said that they hated it. Therefore the sentiment is \"negative\".\n",
      "\n",
      "Text: <article> \n",
      "The answer is: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_article = \"<article>\"\n",
    "template_prompt = f\"\"\"Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
    "\n",
    "Text: My new pillow is so soft and fluffy! I hate it.\n",
    "The answer is: The reviewer initially praised the pillow but then said that they hated it. Therefore the sentiment is \"negative\".\n",
    "\n",
    "Text: {selected_article} \n",
    "The answer is: \n",
    "\"\"\"\n",
    "\n",
    "print(template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_article = \"I really loved the new Spider-Man movie. The sub-par animation really made it amazing!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reviewer appears to be using sarcasm in the review, as they mentioned loving the movie but then criticized the animation. Therefore, the sentiment is \"negative\" or potentially \"mixed/neutral\" if considering the sarcasm as humorous.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "chatbot_response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=messages,\n",
    "  temperature=1,\n",
    "  max_tokens=1500,\n",
    ")\n",
    "\n",
    "print(chatbot_response.choices[0].message[\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot better! The model has recognized the sarcasm there and predicts the right sentiment!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Spaces & Evaluation\n",
    "\n",
    "A natural question to ask is - how do we evaluate these models?\n",
    "\n",
    "For Natural Language Generation (NLG) tasks, the set of possible answers is unconstrained. That is, anything the model generates could potentially be an answer. There are a couple of methods to evaluate this, some of which we go into more detail on in day 2. \n",
    "\n",
    "For Natural Language Understanding (NLU), the set of possible answers is constrained. That is, there is a finite set of answers that we care about. For instance, negative, positive or neutral for sentiment analysis. We can then evaluate these models with tried and tested metrics like Precision and/or Recall. So how do we get these answers?\n",
    "\n",
    "For open source models or models that we have direct access to, we can ensure we get only the probabilities of the answers we want by calculating the conditional probability of each of the possible answers. That is, for a given answer X, we calculate the probability `P(X|input)` for all possible answers. The highest probability then corresponds to the prediction.\n",
    "\n",
    "However, this approach is not possible for closed source models where we only receive direct predictions. While we unfortunately don't have access to the actual confidence of a prediction, we can still get the prediction itself and evaluate the model. Consider the following prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the sentiment of the following review as positive, negative or neutral.\n",
      "\n",
      "Text: <article> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_article = \"<article>\"\n",
    "template_prompt = f\"\"\"Classify the sentiment of the following review as positive, negative or neutral.\n",
    "\n",
    "Text: {selected_article} \n",
    "\"\"\"\n",
    "\n",
    "print(template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_article = \"I loved the new Spider-Man movie!! The animation was really fluid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the following review is positive.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "chatbot_response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=messages,\n",
    "  temperature=1,\n",
    "  max_tokens=1500,\n",
    ")\n",
    "\n",
    "print(chatbot_response.choices[0].message[\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the answer we wanted but there's a extra text that makes it hard to work with! If we change the prompt a little to reflect our original prompt that we used above, we get single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
      "\n",
      "Text: <article> \n",
      "The answer is: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_article = \"<article>\"\n",
    "template_prompt = f\"\"\"Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
    "\n",
    "Text: {selected_article} \n",
    "The answer is: \n",
    "\"\"\"\n",
    "\n",
    "print(template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_article = \"I loved the new Spider-Man movie!! The animation was really fluid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "prompt = template_prompt.replace('<article>', selected_article)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "chatbot_response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=messages,\n",
    "  temperature=1,\n",
    "  max_tokens=1500,\n",
    ")\n",
    "\n",
    "print(chatbot_response.choices[0].message[\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus by adjusting our prompts a little, we can nudge the model to give the outputs we want it to give in the format that we prefer. Now it's just a matter of compiling the model outputs for a number of different examples and compiling overall statistics!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-bootcamp-4wh1UwyX-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
