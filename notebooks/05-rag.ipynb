{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We've abstracted away the code from the previous notebooks to focus on the concepts from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "data = utils.load_data(sample_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "We've abstracted away the code from the previous notebooks to focus on the concepts from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2515.73it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = utils.preprocess_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "We've abstracted away the code from the previous notebooks to focus on the concepts from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b431040bf04c83a1414485ae7247a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents before chunking: 100\n",
      "Documents after chunking: 852\n"
     ]
    }
   ],
   "source": [
    "nodes = utils.chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "We've abstracted away the code from the previous notebooks to focus on the concepts from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashsaravanan/Library/Caches/pypoetry/virtualenvs/genai-bootcamp-4wh1UwyX-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = utils.get_embedding_model(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "\n",
    "We use the same indexes from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c57ebd83fd4143b3696bdb4cabf69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/852 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Recreate the flat in-memory index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex(nodes, embed_model=embedding_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ANN index we created in the previous notebook from disk\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "vector_store = LanceDBVectorStore(\n",
    "    uri=\"./lancedb\", table_name=\"test\"\n",
    ")\n",
    "vdb_index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG\n",
    "\n",
    "So now we have an index and a way to query the index. To close the RAG loop here, we need to get an input from a user, pass it to the retriever, send the retrieved context to the LLM, and return the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"How many points did Michael Jordan actually score in his final NBA game?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "# Build our prompt\n",
    "system_prompt = \"You are a helpful AI assistant that answers questions and always cites sources (document titles & ID) as well as the relevant snippets. If the information is not present in the context, you will say 'I don't know'.\"\n",
    "\n",
    "system_prompt = ChatMessage(content=system_prompt, role=\"system\")\n",
    "\n",
    "prompt = \"\"\"Context: \n",
    "{context_str} \n",
    "-----\n",
    "Question: {query_str} \n",
    "Answer: `answer`\n",
    "Source: `source`\n",
    "Relevant Snippet: `snippet`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Choice\n",
    "\n",
    "This demo supports two different APIs for models - OpenAI and AnyScale. Specifically, we use `gpt-4o` and `meta-llama/Meta-Llama-3-70B-Instruct` but any supported model within these two environments should work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(index, user_input):\n",
    "    results = index.as_retriever(similarity_top_k=3).retrieve(user_input)\n",
    "    return \"\\n----------------\\n\".join([f\"Title: {result.metadata['title']}\\nDocument ID: {result.node_id}\\n{result.text}\" for result in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Jordan scored 13 points in his final NBA game.\n",
      "\n",
      "Source: Michael Jordan (Document ID: 4ecaf67a-0337-46b6-ba38-c78f871b293c)\n",
      "Relevant Snippet: \"Jordan's final NBA game was on April 16, 2003 in Philadelphia. After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and with his team trailing the Philadelphia 76ers, 75–56.\"\n"
     ]
    }
   ],
   "source": [
    "# OpenAI + Flat Index\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Set up OpenAI client - API key is handled in your .env file\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0.1, max_tokens=2048)\n",
    "\n",
    "# Run retrieval\n",
    "context = retrieve(index, user_input)\n",
    "\n",
    "# Generate output\n",
    "message = ChatMessage(content=prompt.format(context_str=context, query_str=user_input), role=\"user\")\n",
    "print(llm.chat([system_prompt, message]).message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Jordan scored 15 points in his final NBA game.\n",
      "\n",
      "Source: Michael Jordan (Document ID: 5ab87d87-a790-4c80-b8bd-d3b9525e442d)\n",
      "Relevant Snippet: \"Jordan's final NBA game was on April 16, 2003 in Philadelphia. After scoring only 13 points in the game... At 1:45, Jordan was intentionally fouled by the 76ers' Eric Snow, and stepped to the line to make both free throws.\"\n"
     ]
    }
   ],
   "source": [
    "# OpenAI + ANN Index\n",
    "\n",
    "# Set up OpenAI client - API key is handled in your .env file\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0.1, max_tokens=2048)\n",
    "\n",
    "# Run retrieval\n",
    "context = retrieve(vdb_index, user_input)\n",
    "\n",
    "# Generate output\n",
    "message = ChatMessage(content=prompt.format(context_str=context, query_str=user_input), role=\"user\")\n",
    "print(llm.chat([system_prompt, message]).message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 13\n",
      "Source: Michael Jordan (Document ID: 4ecaf67a-0337-46b6-ba38-c78f871b293c)\n",
      "Relevant Snippet: \"After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and with his team trailing the Philadelphia 76ers, 75–56.\"\n"
     ]
    }
   ],
   "source": [
    "# AnyScale + Flat Index\n",
    "from llama_index.llms.anyscale import Anyscale\n",
    "\n",
    "# Set up AnyScale client - API key is handled in your .env file\n",
    "llm = Anyscale(model=\"meta-llama/Meta-Llama-3-70B-Instruct\", temperature=0.1, max_tokens=2048)\n",
    "\n",
    "# Run retrieval\n",
    "context = retrieve(index, user_input)\n",
    "\n",
    "# Generate output\n",
    "message = ChatMessage(content=prompt.format(context_str=context, query_str=user_input), role=\"user\")\n",
    "print(llm.chat([system_prompt, message]).message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 13 points\n",
      "Source: Michael Jordan (Document ID: 5ab87d87-a790-4c80-b8bd-d3b9525e442d)\n",
      "Relevant Snippet: \"After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and with his team trailing the Philadelphia 76ers, 75–56.\"\n"
     ]
    }
   ],
   "source": [
    "# AnyScale + ANN Index\n",
    "from llama_index.llms.anyscale import Anyscale\n",
    "\n",
    "# Set up AnyScale client - API key is handled in your .env file\n",
    "llm = Anyscale(model=\"meta-llama/Meta-Llama-3-70B-Instruct\", temperature=0.1, max_tokens=2048)\n",
    "\n",
    "# Run retrieval\n",
    "context = retrieve(vdb_index, user_input)\n",
    "\n",
    "# Generate output\n",
    "message = ChatMessage(content=prompt.format(context_str=context, query_str=user_input), role=\"user\")\n",
    "print(llm.chat([system_prompt, message]).message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-bootcamp-4wh1UwyX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
