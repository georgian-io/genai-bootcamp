{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We've abstracted away the code from the previous notebooks to focus on the concepts from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "data = utils.load_data(sample_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "We've abstracted away the code from the previous notebooks to focus on the concepts from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2821.58it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = utils.preprocess_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "We've abstracted away the code from the previous notebooks to focus on the concepts from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d62e783af5149eda63bdb54ce28a515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents before chunking: 100\n",
      "Documents after chunking: 837\n"
     ]
    }
   ],
   "source": [
    "nodes = utils.chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "We've abstracted away the code from the previous notebooks to focus on the concepts from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashsaravanan/Library/Caches/pypoetry/virtualenvs/genai-bootcamp-4wh1UwyX-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = utils.get_embedding_model(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "\n",
    "We use the same indexes from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdef06e9f11b40d6b68ea04faf146372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Recreate the flat in-memory index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex(nodes, embed_model=embedding_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ANN index we created in the previous notebook from disk\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "vector_store = LanceDBVectorStore(\n",
    "    uri=\"./lancedb\", table_name=\"test\"\n",
    ")\n",
    "vdb_index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG\n",
    "\n",
    "So now we have an index and a way to query the index. To close the RAG loop here, we need to get an input from a user, pass it to the retriever, send the retrieved context to the LLM, and return the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"How many points did Michael Jordan actually score in his final NBA game?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "# Build our prompt\n",
    "system_prompt = \"You are a helpful AI assistant that answers questions after carefully reading all the provided context. You always cite sources (document titles) and also quote the relevant snippets. Information may be spread across multiple documents. If the information is not present in any of the contexts, you will say 'I don't know'.\"\n",
    "\n",
    "prompt = \"\"\"Context: \n",
    "{context_str} \n",
    "-----\n",
    "Question: {query_str} \n",
    "Answer: `answer`\n",
    "Source: `source`\n",
    "Relevant Snippet: `snippet`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Choice\n",
    "\n",
    "This demo supports two different APIs for models - OpenAI and AnyScale. Specifically, we use `gpt-4o` and `meta-llama/Meta-Llama-3-70B-Instruct` but any supported model within these two environments should work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(index, user_input):\n",
    "    results = index.as_retriever(similarity_top_k=3).retrieve(user_input)\n",
    "    return \"\\n----------------\\n\".join([f\"Title: {result.metadata['title']}\\n{result.text}\" for result in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(llm, index, system_prompt, prompt, user_input):\n",
    "    # Retrieve\n",
    "    context = retrieve(index, user_input)\n",
    "    # Augment\n",
    "    formatted_prompt = prompt.format(context_str=context, query_str=user_input)\n",
    "    # Generate\n",
    "    messages = [\n",
    "        ChatMessage(content=system_prompt, role=\"system\"),\n",
    "        ChatMessage(content=formatted_prompt, role=\"user\")\n",
    "    ]\n",
    "    result = llm.chat(messages)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Michael Jordan scored 15 points in his final NBA game.\n",
      "\n",
      "Source: Michael Jordan\n",
      "\n",
      "Relevant Snippet: \"Jordan's final NBA game was on April 16, 2003 in Philadelphia. After scoring only 13 points in the game, Jordan went to the bench... Jordan finally rose from the bench and re-entered the game... At 1:45, Jordan was intentionally fouled by the 76ers' Eric Snow, and stepped to the line to make both free throws.\"\n"
     ]
    }
   ],
   "source": [
    "# OpenAI + Flat Index\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Set up OpenAI client - API key is handled in your .env file\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0.1, max_tokens=2048)\n",
    "\n",
    "# Generate output\n",
    "print(rag(llm, index, system_prompt, prompt, user_input).message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Michael Jordan scored 15 points in his final NBA game.\n",
      "\n",
      "Source: Michael Jordan\n",
      "\n",
      "Relevant Snippet: \"After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter... At 1:45, Jordan was intentionally fouled by the 76ers' Eric Snow, and stepped to the line to make both free throws.\"\n"
     ]
    }
   ],
   "source": [
    "# OpenAI + ANN Index\n",
    "\n",
    "# Set up OpenAI client - API key is handled in your .env file\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0.1, max_tokens=2048)\n",
    "\n",
    "# Generate output\n",
    "print(rag(llm, vdb_index, system_prompt, prompt, user_input).message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 13 points\n",
      "Source: Michael Jordan\n",
      "Relevant Snippet: \"After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and with his team trailing the Philadelphia 76ers, 75–56.\"\n"
     ]
    }
   ],
   "source": [
    "# AnyScale + Flat Index\n",
    "from llama_index.llms.anyscale import Anyscale\n",
    "\n",
    "# Set up AnyScale client - API key is handled in your .env file\n",
    "llm = Anyscale(model=\"meta-llama/Meta-Llama-3-70B-Instruct\", temperature=0.1, max_tokens=2048)\n",
    "\n",
    "# Generate output\n",
    "print(rag(llm, index, system_prompt, prompt, user_input).message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 13 points\n",
      "Source: Michael Jordan\n",
      "Relevant Snippet: \"After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and with his team trailing the Philadelphia 76ers, 75–56.\"\n"
     ]
    }
   ],
   "source": [
    "# AnyScale + ANN Index\n",
    "from llama_index.llms.anyscale import Anyscale\n",
    "\n",
    "# Set up AnyScale client - API key is handled in your .env file\n",
    "llm = Anyscale(model=\"meta-llama/Meta-Llama-3-70B-Instruct\", temperature=0.1, max_tokens=2048)\n",
    "\n",
    "# Generate output\n",
    "print(rag(llm, vdb_index, system_prompt, prompt, user_input).message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-bootcamp-4wh1UwyX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
