{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"EleutherAI/wikitext_document_level\", \"wikitext-103-raw-v1\", trust_remote_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "data_index = 38\n",
    "np.random.seed(42)\n",
    "data = data[\"train\"][:sample_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "import re\n",
    "pattern = re.compile(r\" @(.)@ \")\n",
    "\n",
    "# Run this across the entire dataset\n",
    "for i, page in enumerate(data[\"page\"]):\n",
    "    data[\"page\"][i] = re.sub(pattern, r\"\\1\", page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data enrichment\n",
    "def extract_metadata(data):\n",
    "    title_pattern = re.compile(r\"\\s=\\s([^=]{1,50})\\s=\\s\")\n",
    "    title = [item for item in re.findall(title_pattern, data)]\n",
    "    # The regex above isn't perfect so we take the first match as the title \n",
    "    if len(title) > 0:\n",
    "        title = title[0]\n",
    "    else:\n",
    "        title = \"Unknown Title\"\n",
    "    return {\"title\": title}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4140.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load documents\n",
    "from llama_index.core import Document\n",
    "\n",
    "documents = []\n",
    "for i in tqdm.tqdm(range(len(data[\"page\"]))):\n",
    "    documents.append(\n",
    "        Document(\n",
    "            text=data[\"page\"][i],\n",
    "            metadata=extract_metadata(data[\"page\"][i]),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "chunker = SentenceSplitter(chunk_size=512, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23428e5d3f764f299979f9d3a8a82813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nodes = chunker.get_nodes_from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents before chunking: 100\n",
      "Documents after chunking: 837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextNode(id_='e697c15b-1485-4fc9-9721-b93ce6b9a91e', embedding=None, metadata={'title': 'Valkyria Chronicles III'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='23b40453-88f9-4896-b836-02fa1bda18a9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'Valkyria Chronicles III'}, hash='f7aadfb478d20e04be770cd882b5e6a44c185eb28a53810838586313c39ccc7c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='48905c58-4673-464a-8f2d-d9be3d0604b9', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='32c32274359d6ec7e58a31e940b4b433c53354c2fb60611c7cc6bd2c324d075c')}, text='= Valkyria Chronicles III = \\n \\n Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role-playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real-time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game \\'s opening theme was sung by May \\'n . \\n It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game \\'s expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n \\n = = Gameplay = = \\n \\n As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role-playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book-like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text .', start_char_idx=1, end_char_idx=2234, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Documents before chunking: {len(documents)}\")\n",
    "print(f\"Documents after chunking: {len(nodes)}\")\n",
    "nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashsaravanan/Library/Caches/pypoetry/virtualenvs/genai-bootcamp-4wh1UwyX-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", embed_batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc0a9fcf38e445bbef4177efdba44d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex(nodes, embed_model=embedding_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorstore / Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/examples/vector_stores/LanceDBIndexDemo/\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# Create your DB locally\n",
    "vector_store = LanceDBVectorStore(\n",
    "    uri=\"./lancedb\", table_name=\"test\"\n",
    ")\n",
    "# Link to the collection on llamaindex\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the index we created in the previous notebook\n",
    "vector_store = LanceDBVectorStore(\n",
    "    uri=\"./lancedb\", table_name=\"test\"\n",
    ")\n",
    "vdb_index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG\n",
    "\n",
    "So now we have an index and a way to query the index. To close the RAG loop here, we need to get an input from a user, pass it to the retriever, send the retrieved context to the LLM, and return the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"How many points did Michael Jordan actually score in his final NBA game?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "# Build our prompt\n",
    "system_prompt = \"You are a helpful AI assistant that answers questions and always cites sources (document titles & ID) as well as the relevant snippets. If the information is not present in the context, you will say 'I don't know'.\"\n",
    "\n",
    "system_prompt = ChatMessage(content=system_prompt, role=\"system\")\n",
    "\n",
    "prompt = \"\"\"Context: \n",
    "{context} \n",
    "-----\n",
    "Question: {question} \n",
    "Answer: `answer`\n",
    "Source: `source`\n",
    "Relevant Snippet: `snippet`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Choice\n",
    "\n",
    "This demo supports two different APIs for models - OpenAI and AnyScale. Specifically, we use `gpt-4o` and `meta-llama/Meta-Llama-3-70B-Instruct` but any supported model within these two environments should work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(index, user_input):\n",
    "    results = index.as_retriever(similarity_top_k=3).retrieve(user_input)\n",
    "    return \"\\n----------------\\n\".join([f\"Title: {result.metadata['title']}\\nDocument ID: {result.node_id}\\n{result.text}\" for result in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Jordan scored 15 points in his final NBA game.\n",
      "\n",
      "Source: Michael Jordan (Document ID: 908cf6b2-8c31-4219-baa2-50bbeb9a351a)\n",
      "Relevant Snippet: \"After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter... At 1:45, Jordan was intentionally fouled by the 76ers' Eric Snow, and stepped to the line to make both free throws.\"\n"
     ]
    }
   ],
   "source": [
    "# OpenAI + Flat Index\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Set up OpenAI client - API key is handled in your .env file\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0.1, max_tokens=2048)\n",
    "\n",
    "# Run retrieval\n",
    "context = retrieve(index, user_input)\n",
    "\n",
    "# Generate output\n",
    "message = ChatMessage(content=prompt.format(context=context, question=user_input), role=\"user\")\n",
    "print(llm.chat([system_prompt, message]).message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Jordan scored 15 points in his final NBA game.\n",
      "\n",
      "Source: Michael Jordan (Document ID: 5ab87d87-a790-4c80-b8bd-d3b9525e442d)\n",
      "Relevant Snippet: \"After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter... At 1:45, Jordan was intentionally fouled by the 76ers' Eric Snow, and stepped to the line to make both free throws.\"\n"
     ]
    }
   ],
   "source": [
    "# OpenAI + ANN Index\n",
    "\n",
    "# Set up OpenAI client - API key is handled in your .env file\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0.1, max_tokens=2048)\n",
    "\n",
    "# Run retrieval\n",
    "context = retrieve(vdb_index, user_input)\n",
    "\n",
    "# Generate output\n",
    "message = ChatMessage(content=prompt.format(context=context, question=user_input), role=\"user\")\n",
    "print(llm.chat([system_prompt, message]).message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 13 points\n",
      "Source: Michael Jordan (Document ID: 908cf6b2-8c31-4219-baa2-50bbeb9a351a)\n",
      "Relevant Snippet: \"After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and with his team trailing the Philadelphia 76ers, 75–56.\"\n"
     ]
    }
   ],
   "source": [
    "# AnyScale + Flat Index\n",
    "from llama_index.llms.anyscale import Anyscale\n",
    "\n",
    "# Set up AnyScale client - API key is handled in your .env file\n",
    "llm = Anyscale(model=\"meta-llama/Meta-Llama-3-70B-Instruct\", temperature=0.1, max_tokens=2048)\n",
    "\n",
    "# Run retrieval\n",
    "context = retrieve(index, user_input)\n",
    "\n",
    "# Generate output\n",
    "message = ChatMessage(content=prompt.format(context=context, question=user_input), role=\"user\")\n",
    "print(llm.chat([system_prompt, message]).message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 13 points\n",
      "Source: Michael Jordan (Document ID: 5ab87d87-a790-4c80-b8bd-d3b9525e442d)\n",
      "Relevant Snippet: \"After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and with his team trailing the Philadelphia 76ers, 75–56.\"\n"
     ]
    }
   ],
   "source": [
    "# AnyScale + ANN Index\n",
    "from llama_index.llms.anyscale import Anyscale\n",
    "\n",
    "# Set up AnyScale client - API key is handled in your .env file\n",
    "llm = Anyscale(model=\"meta-llama/Meta-Llama-3-70B-Instruct\", temperature=0.1, max_tokens=2048)\n",
    "\n",
    "# Run retrieval\n",
    "context = retrieve(vdb_index, user_input)\n",
    "\n",
    "# Generate output\n",
    "message = ChatMessage(content=prompt.format(context=context, question=user_input), role=\"user\")\n",
    "print(llm.chat([system_prompt, message]).message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-bootcamp-4wh1UwyX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
